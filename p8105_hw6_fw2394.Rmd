---
title: "p8105_hw6_fw2394"
author: "Fang Wang"
date: "2024-11-29"
output: github_document
---
# Load libraries and setting up
```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
library(purrr) # for map
library(modelr)
library(rsample) # Load the rsample package for cross-validation
library(Metrics)  # for rmse()

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp =.6,
  out.width = "90%"
)

theme_set (theme_bw() + theme (
    legend.position =  "bottom"
  ))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_color_discrete = scale_color_viridis_d()
scale_fill_discrete = scale_fill_viridis_d()
```

# Problem 1

## Load data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```
*The dataset contains 365 observations and 6 variables.*

## run linear regression between tmax and tmin
```{r}
lm(tmax ~ tmin, data = weather_df) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)


lm(tmax ~ tmin, data = weather_df) |> 
  broom::glance(model) |> 
  knitr::kable(digits = 3)
```
## write a function to draw a bootstrap sample
```{r}
boot_sample = function (df){
  sample_frac(df, replace = TRUE)
}
```

## draw one boot sample and run linear model
```{r}
boot_sample(weather_df) |> 
  lm(tmax ~ tmin, data =_) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
# Unless I set seed, otherwise the estimates and slopes are different for each draw.

boot_sample(weather_df) |> # draw 365 observations but may draw the same observations more than once. 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm") +
  labs(title = "Association Between Minimal Temperature (tmin) and Maximum Temperature (tmax) ")
```
*The linear regression line has a positive slope, meaning there is a positive relationship between the minimal temperature and the maximum temperature. In simpler terms, as the minimum temperature increases, the maximum temperature tends to increase as well.*

## draw 5000 bootstrap samples
```{r}
boot_straps =
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_samples = map (strap_number, \(i) boot_sample(df = weather_df))
  )
```

## Calculate estimates and rsquare from 5000 bootstrap samples
```{r}
bootstrap_results = 
  boot_straps |> 
  mutate(
    models = map(strap_samples, \(df) lm(tmax ~ tmin, data = df) ),
    results_estimates = map(models, broom::tidy),
    rsquare = map(models, broom::glance))|> 
  select(-strap_samples, -models) |> 
  unnest(results_estimates, rsquare) 

estimate_rsquare_df = bootstrap_results |> 
  janitor::clean_names() |> 
  select (strap_number, term, estimate, r_squared)
```
*The dataset contains 10000 observations and 4 variables, including strap_number, term, estimate and r_squared.*

## Change format of the dataset for plotting:
```{r}
estimate_rsquare_df_wider = estimate_rsquare_df |> 
  pivot_wider(
    names_from = term, 
    values_from = estimate
  ) |> 
  rename(
    estimate_intercept = `(Intercept)`,  # Wrap (Intercept) in backticks
    estimate_beta = tmin
  )
```

## Plot the distribution of log(β0 * β1)
```{r}
estimate_rsquare_df_wider |> 
  ggplot(aes(x = log(estimate_intercept*estimate_beta)))+
  geom_histogram(bins =30, color = "black", fill = "pink") +
  labs (
    title = "Distribution of log(β0 * β1) for 5000 bootstrap samples",
    x = "log(β0 * β1)",
    y = "Frequency"
  ) +
  theme(axis.text.x = element_text(hjust = 1)) +
  theme(plot.title=element_text(size=18, face="bold"))+
  theme(plot.title = element_text(hjust = 0.5))
```
*The center of the distribution indicates that the product of β₀ and β₁ is most likely around 2.00, and the histogram suggests that this estimate is consistent with relatively little spread.*

## Plot the distribution of rsquare
```{r}
estimate_rsquare_df_wider |> 
  ggplot(aes(x = r_squared))+
  geom_histogram(bins =30, color = "black", fill = "lightblue") +
  labs (
    title = "Distribution of R square for 5000 bootstrap samples",
    x = "R square",
    y = "Frequency"
  ) +
  theme(axis.text.x = element_text(hjust = 1)) +
  theme(plot.title=element_text(size=18, face="bold"))+
  theme(plot.title = element_text(hjust = 0.5))
```
*The model seems to perform consistently well across bootstrap samples, with an average R² of around 0.91, indicating that it explains 91% of the variance in the response variable. The narrow distribution and symmetry suggest reliable model performance, and the relatively high R² values suggest strong predictive power.*

## calcuate 95% confidence interval for rsquare
```{r}
estimate_rsquare_df_wider |> 
  summarize(
    ci_lower = quantile(r_squared, 0.025), 
    ci_upper = quantile(r_squared, 0.975))
```
*95% confidence interval for rsquare is (0.894, 0.927).*

## calcuate 95% confidence interval for log(β0 * β1)
```{r}
estimate_rsquare_df_wider |> 
summarize(
    ci_lower = quantile(log(estimate_intercept*estimate_beta), 0.025), 
    ci_upper = quantile(log(estimate_intercept*estimate_beta), 0.975))
```
*95% confidence interval for log(β0_β1) is (1.97, 2.06).*


# Problem 2

## Load and clean data
```{r}
homicide = read.csv("./data/homicide-data.csv")|> 
  mutate(city_state = paste(city, state, sep =",")) |> 
  filter(!(city_state%in%c("Dallas,TX", "Phoenix,AZ", "Kansas City,MO", "Tulsa,AL"))) |> 
  mutate(victim_age = as.numeric(victim_age)) |> 
  filter (victim_race %in% c("White", "Black")) |> 
  mutate(
    solved = case_when(
    disposition %in% c("Closed without arrest", "Closed by arrest") ~ 1, # Solved cases
    disposition == "Open/No arrest" ~ 0,                                # Unsolved cases
    TRUE ~ NA_integer_                                                  # Default (optional, NA if unexpected values)
    )
  )
```
*The dataset contains 39693 observations and 14 variables.*

## run glm for Baltimore, MD
```{r}
# filter data for Baltimore,MD
baltimore_df = homicide |> 
  filter(city_state == "Baltimore,MD") |> 
  mutate(victim_sex = as.factor (victim_sex),
         victim_race = as.factor (victim_race))

# run generalized linear models
baltimore_model = glm(solved ~ victim_age + victim_sex + victim_race, family = binomial(), data = baltimore_df)

## calculate the odds ratio in the model
model_summary = tidy(baltimore_model, conf.int = TRUE, exponentiate = TRUE)
model_summary
```

## Extract the odds ratio for male vs. female victims
```{r}
male_female_odds_ratio = model_summary|> 
  filter(term == "victim_sexMale") |> 
  select(estimate, conf.low, conf.high) |> 
  janitor::clean_names()

male_female_odds_ratio
```
*The estimate of victim_sexMale is 0.355 that indicates the odds of solving homicides for male victims are 64.5% lower than for female victims (1−0.355=0.645), holding all other factors constant. Its confidence Interval (0.267, 0.468) means the true odds are estimated to be between 0.267 and 0.468. p-value: 3.74×10^(−13), Highly statistically significant (p<0.001).*

## run glm for each city
```{r}
homicide_cities = homicide|> 
  group_by(city_state) |> 
  nest() |>  # a nested list-column
  mutate(
    models = map(data, ~glm(solved ~ victim_age + victim_sex + victim_race, family = binomial(), data =.x)),
    results = map (models, ~tidy(.x, conf.int = TRUE, exponentiate = TRUE))
  ) |> 
  select (city_state, results) |> 
  unnest(results) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, term, estimate, conf.low, conf.high) |> 
  janitor::clean_names()
```
*Homicide_cities contains 47 observations/cities and 5 variables, including state_city, term, estimate, conf_low and conf_high.*

## plot ORs and Cls
```{r}
homicide_cities |> 
  ggplot (aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf_low, ymax= conf_high)) + # Add confidence intervals
  geom_hline(yintercept = 1, linetype = 'dashed', color = 'gray') + # Add a dashed line at odds ratio = 1
  labs(x = 'City', y = 'Odds Ratio', title = 'Estimated Odds Ratios and Confidence Intervals for Each City')+  # Labels and title
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  
```
*The plot provides a comparison of the odds ratios for different cities, allowing us to see which cities have a higher or lower likelihood of experiencing the event compared to the baseline. Cities like New York, NY and Long Beach, CA have odds ratios less than 1, which suggests that solving homicides is less likely in these cities compared to the reference group. However, cities like Fresno, CA and Minneapolis, MN have odds ratios greater than 1, meaning that the event is less likely in these cities compared to the reference group.*